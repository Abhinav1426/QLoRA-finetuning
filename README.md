# Resume Optimizer - QLoRA Fine-tuned Qwen2.5-3B

A fine-tuned language model for optimizing resumes based on job descriptions with ATS (Applicant Tracking System) scoring. This project uses QLoRA (Quantized Low-Rank Adaptation) to efficiently fine-tune the Qwen2.5-3B-Instruct model.

## üìã Overview

This project fine-tunes a 3B parameter language model to:
- **Optimize resumes** to align with specific job descriptions
- **Generate structured JSON** output with optimized resume content
- **Calculate ATS scores** for both original and optimized resumes
- **Provide improvement metrics** to quantify resume enhancement

The model is trained using QLoRA, enabling efficient fine-tuning on consumer GPUs (tested on NVIDIA RTX 4070 with 24GB VRAM).

## üöÄ Features

- **QLoRA Training**: Memory-efficient 4-bit quantization with LoRA adapters
- **Structured Output**: Generates valid JSON with resume data and ATS scores
- **Dataset Processing**: Automated data sanitization and preprocessing
- **Validation Pipeline**: Built-in JSON validation and quality checks
- **GGUF Export**: Convert fine-tuned model to GGUF format for deployment
- **Ollama Integration**: Ready-to-use Modelfile for local deployment

## üìÅ Project Structure

```
QLoRA-finetuning/
‚îú‚îÄ‚îÄ Resume_Optimizer_QLoRA_Qwen25_Training.ipynb  # Main training notebook
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îî‚îÄ‚îÄ large_dataset_20251103_231545.jsonl       # Training data (1,304 examples)
‚îú‚îÄ‚îÄ qwen25_resume_lora/                            # LoRA adapter weights (generated)
‚îú‚îÄ‚îÄ qwen25_resume_merged/                          # Merged model (generated)
‚îú‚îÄ‚îÄ environment.yml                                # Conda environment
‚îú‚îÄ‚îÄ .gitignore                                     # Git ignore rules
‚îî‚îÄ‚îÄ README.md                                      # This file
```

## üõ†Ô∏è Installation

### Prerequisites

- **Python**: 3.11+
- **CUDA**: 11.8+ (for GPU acceleration)
- **GPU**: NVIDIA GPU with 24GB+ VRAM recommended
- **Conda**: For environment management

### Setup Environment

1. **Clone the repository**
```bash
git clone https://github.com/Abhinav1426/QLoRA-finetuning.git
cd QLoRA-finetuning
```

2. **Create conda environment**
```bash
conda env create -f environment.yml
conda activate finetune
```

Or install dependencies manually:
```bash
conda create -n finetune python=3.11
conda activate finetune
pip install "transformers>=4.44" "datasets" "accelerate" "trl" "peft" "bitsandbytes"
```

3. **Verify GPU availability**
```python
import torch
print(torch.cuda.is_available())  # Should return True
print(torch.cuda.get_device_name(0))
```

## üìä Dataset Format

The training dataset uses JSONL format with the following structure:

```json
{
  "resume_text": "Original resume content...",
  "job_description": "Target job description...",
  "optimized_resume_text": "Optimized resume text...",
  "optimized_resume_json": {
    "contact": {...},
    "experiences": [...],
    "skills": [...]
  },
  "ats_score_original": 65,
  "ats_score_regenerated": 88,
  "improvement": 23
}
```

**Dataset Statistics:**
- Total examples: 1,304
- Train split: 912 (70%)
- Validation split: 196 (15%)
- Test split: 196 (15%)

## üéØ Training

### Quick Start

Open `Resume_Optimizer_QLoRA_Qwen25_Training.ipynb` and run all cells sequentially:

1. **Section 1-3**: Load and preprocess dataset
2. **Section 4**: Load Qwen2.5-3B with QLoRA configuration
3. **Section 5**: Train the model (3 epochs, ~2-3 hours on RTX 4070)
4. **Section 6**: Validate JSON output quality
5. **Section 7-8**: Save and reload model for inference

### Training Configuration

```python
Model: Qwen/Qwen2.5-3B-Instruct
Quantization: 4-bit NF4 with double quantization
LoRA Config:
  - Rank (r): 32
  - Alpha: 32
  - Dropout: 0.05
  - Target modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

Training Parameters:
  - Batch size: 1 (per device)
  - Gradient accumulation: 16 steps
  - Effective batch size: 16
  - Learning rate: 2e-5
  - Epochs: 3
  - Max sequence length: 1024 tokens
  - Optimizer: AdamW with cosine scheduling
```

### Memory Requirements

- **Model loading**: ~4-5 GB VRAM (4-bit quantized)
- **Training peak**: ~20-22 GB VRAM
- **Inference**: ~4-5 GB VRAM

### Training Time

- **RTX 4070 (24GB)**: ~2-3 hours for 3 epochs
- **RTX 3090 (24GB)**: ~2.5-3.5 hours for 3 epochs
- **A100 (40GB)**: ~1-1.5 hours for 3 epochs

## üîç Usage

### Inference with LoRA Adapter

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    quantization_config=bnb_config,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, "./qwen25_resume_lora")
tokenizer = AutoTokenizer.from_pretrained("./qwen25_resume_lora")

# Generate
prompt = """You are an expert resume optimization and ATS analysis engine.
Resume text: [YOUR RESUME]
Job description: [JOB DESCRIPTION]"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=1024)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### Expected Output Format

```json
{
  "optimized_resume_json": {
    "contact": {
      "name": "John Doe",
      "email": "john@example.com",
      "phone": "+1234567890"
    },
    "experiences": [
      {
        "title": "Software Engineer",
        "company": "Tech Corp",
        "duration": "2020-2023",
        "description": ["Built scalable APIs", "Led team of 5 developers"]
      }
    ],
    "skills": ["Python", "Machine Learning", "AWS"]
  },
  "optimized_resume_text": "Full optimized resume text...",
  "ats_score_original": 65,
  "ats_score_regenerated": 88,
  "improvement": 23
}
```

## üì¶ Model Export

### Convert to GGUF Format

The notebook includes a complete workflow (Section 9) to convert the model to GGUF format for deployment with llama.cpp or Ollama.

**Step 1: Merge LoRA adapter** (Run Section 9.1)
```python
# Merges LoRA weights into base model
# Output: ./qwen25_resume_merged/
```

**Step 2: Convert to GGUF** (Terminal)
```bash
# Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
pip install -r requirements.txt

# Convert to GGUF fp16
python convert_hf_to_gguf.py ../qwen25_resume_merged \
  --outfile ../qwen25_resume_gguf/model-f16.gguf

# Quantize to Q4_K_M (recommended: 2GB, good quality)
./llama-quantize \
  ../qwen25_resume_gguf/model-f16.gguf \
  ../qwen25_resume_gguf/model-q4_k_m.gguf \
  Q4_K_M
```

**Quantization Options:**
| Format | Size | Quality | Use Case |
|--------|------|---------|----------|
| fp16   | ~6GB | Best    | High-end deployment |
| Q8_0   | ~3GB | Excellent | Production servers |
| Q4_K_M | ~2GB | Good    | **Recommended for most users** |
| Q2_K   | ~1GB | Fair    | Resource-constrained devices |

### Deploy with Ollama

```bash
# Create Ollama model
ollama create qwen25-resume -f ./Modelfile

# Run inference
ollama run qwen25-resume
```

## üìà Model Performance

### Validation Metrics

- **JSON Parse Success Rate**: >95% on validation set
- **Structure Validation**: >90% pass all required fields
- **Average Generation Time**: ~3-5 seconds per resume (on RTX 4070)
- **ATS Score Improvement**: Average +15-25 points

### Known Limitations

- Maximum input length: 1024 tokens (resume + job description)
- Occasional JSON formatting issues with very long outputs
- Best performance on technical/professional resumes
- May require prompt engineering for specific industries

## üêõ Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```python
# Solution: Reduce batch size or sequence length
per_device_train_batch_size=1  # Already at minimum
max_length=512  # Reduce from 1024
```

**2. JSON Parse Failures**
```python
# Solution: Use temperature=0.1 for more deterministic output
# Already implemented in validation sections
```

**3. LoRA Adapter Loading Issues**
```python
# Solution: Ensure adapter and base model versions match
# Use the same transformers version for training and inference
```

**4. dtype Mismatch Errors**
```python
# Solution: Use the provided _ensure_float32_lm_head() helper
# Already implemented in the notebook
```

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

- [ ] Support for longer context (2048+ tokens)
- [ ] Multi-language resume optimization
- [ ] Fine-tuning on industry-specific datasets
- [ ] Integration with ATS scoring APIs
- [ ] Web interface for easy inference
- [ ] Batch processing scripts

## üìÑ License

This project is licensed under the MIT License. See LICENSE file for details.

**Note**: The Qwen2.5-3B-Instruct base model has its own license terms. Please review the [Qwen license](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) before commercial use.

## üôè Acknowledgments

- **Qwen Team** for the excellent Qwen2.5-3B-Instruct base model
- **Hugging Face** for transformers, PEFT, and TRL libraries
- **Tim Dettmers** for bitsandbytes quantization
- **llama.cpp** team for GGUF conversion tools

## üìß Contact

- **Author**: Abhinav
- **GitHub**: [@Abhinav1426](https://github.com/Abhinav1426)
- **Repository**: [QLoRA-finetuning](https://github.com/Abhinav1426/QLoRA-finetuning)

## üìö References

- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [Qwen2.5 Technical Report](https://arxiv.org/abs/2309.16609)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
- [GGUF Format Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)

---

**‚≠ê If you find this project helpful, please consider giving it a star!**
