{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63b30ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate finetune\n",
    "# Install core libraries for QLoRA fine-tuning\n",
    "# %pip install \"transformers>=4.44\" \"datasets\" \"accelerate\" \"trl\" \"peft\" \"bitsandbytes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0809a9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4070')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device_name = torch.cuda.get_device_name(0) if cuda_available else \"No GPU detected\"\n",
    "cuda_available, device_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bad7a",
   "metadata": {},
   "source": [
    "# Section 2 — Define the Prompt Template\n",
    "The prompt guides the model to transform a resume and job description into an optimized resume plus ATS scoring JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98b0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are an expert resume optimization and ATS analysis engine.\n",
    "\n",
    "Task:\n",
    "Given a candidate's raw resume text and a job description, you must:\n",
    "1) Rewrite the resume so it is strongly aligned to the job description.\n",
    "2) Produce a structured JSON version of the optimized resume using the schema shown below.\n",
    "3) Assign ATS-style match scores:\n",
    "   - ats_score_original\n",
    "   - ats_score_regenerated\n",
    "4) Compute improvement = ats_score_regenerated - ats_score_original\n",
    "\n",
    "Output rules:\n",
    "- Return ONLY a single valid JSON object.\n",
    "- Use EXACTLY these keys:\n",
    "  \"optimized_resume_json\",\n",
    "  \"optimized_resume_text\",\n",
    "  \"ats_score_original\",\n",
    "  \"ats_score_regenerated\",\n",
    "  \"improvement\"\n",
    "\n",
    "Resume text:\n",
    "<<<RESUME>>>\n",
    "{resume_text}\n",
    "<<<END_RESUME>>>\n",
    "\n",
    "Job description:\n",
    "<<<JOB_DESCRIPTION>>>\n",
    "{job_description}\n",
    "<<<END_JOB_DESCRIPTION>>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e23c4",
   "metadata": {},
   "source": [
    "# Section 3 — Load & Format Dataset\n",
    "The dataset lives under `dataset/large_dataset_20251103_231545.jsonl`; each JSONL row already includes `resume_text`, `job_description`, `optimized_resume_text`, `optimized_resume_json`, `ats_score_original`, `ats_score_regenerated`, and `improvement`. We sanitize each row (coercing any stringified `experiences[].description` fields into lists and skipping malformed JSON entries) before shuffling and splitting 70 %/15 %/15 % for train/validation/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5075cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 1 malformed rows (showing up to 3): [(118, \"Expecting ',' delimiter: line 1 column 50 (char 49)\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['resume_text', 'job_description', 'optimized_resume_text', 'optimized_resume_json', 'ats_score_original', 'ats_score_regenerated', 'improvement'],\n",
       "        num_rows: 912\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['resume_text', 'job_description', 'optimized_resume_text', 'optimized_resume_json', 'ats_score_original', 'ats_score_regenerated', 'improvement'],\n",
       "        num_rows: 196\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['resume_text', 'job_description', 'optimized_resume_text', 'optimized_resume_json', 'ats_score_original', 'ats_score_regenerated', 'improvement'],\n",
       "        num_rows: 196\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"dataset\")\n",
    "DATA_FILE = DATA_DIR / \"large_dataset_20251103_231545.jsonl\"\n",
    "\n",
    "def _coerce_experience_descriptions(exp_list):\n",
    "    if not isinstance(exp_list, list):\n",
    "        return []\n",
    "    cleaned = []\n",
    "    for entry in exp_list:\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        description = entry.get(\"description\")\n",
    "        if isinstance(description, str):\n",
    "            entry[\"description\"] = [description]\n",
    "        elif isinstance(description, list):\n",
    "            entry[\"description\"] = [str(item) for item in description if isinstance(item, (str, int, float))]\n",
    "        else:\n",
    "            entry[\"description\"] = []\n",
    "        cleaned.append(entry)\n",
    "    return cleaned\n",
    "\n",
    "def _sanitize_record(record):\n",
    "    for key in (\"resume_text\", \"job_description\", \"optimized_resume_text\"):\n",
    "        value = record.get(key)\n",
    "        if not isinstance(value, str):\n",
    "            record[key] = \"\" if value is None else str(value)\n",
    "    optimized = record.get(\"optimized_resume_json\")\n",
    "    if not isinstance(optimized, dict):\n",
    "        optimized = {}\n",
    "    else:\n",
    "        optimized[\"experiences\"] = _coerce_experience_descriptions(\n",
    "            optimized.get(\"experiences\", [])\n",
    "        )\n",
    "    record[\"optimized_resume_json\"] = json.dumps(optimized, ensure_ascii=False)\n",
    "    return record\n",
    "\n",
    "records = []\n",
    "skipped_rows = []\n",
    "with DATA_FILE.open(\"r\", encoding=\"utf-8\") as source:\n",
    "    for idx, raw in enumerate(source, start=1):\n",
    "        raw = raw.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        try:\n",
    "            record = json.loads(raw)\n",
    "        except json.JSONDecodeError as exc:\n",
    "            skipped_rows.append((idx, str(exc)))\n",
    "            continue\n",
    "        records.append(_sanitize_record(record))\n",
    "\n",
    "if skipped_rows:\n",
    "    print(f\"Skipped {len(skipped_rows)} malformed rows (showing up to 3): {skipped_rows[:3]}\")\n",
    "\n",
    "full_dataset = Dataset.from_list(records).shuffle(seed=42)\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "val_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "raw_dataset = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"val\": val_test_split[\"train\"],\n",
    "    \"test\": val_test_split[\"test\"],\n",
    "})\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e3ced",
   "metadata": {},
   "source": [
    "## Section 3.1 — Build Prompted Training Examples\n",
    "Define the preprocessing helper that merges each resume/job pair with the standard prompt and serialized target JSON expected by the SFT trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed370db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_target_payload(example):\n",
    "    import json\n",
    "    optimized_json = example.get(\"optimized_resume_json\", \"{}\")\n",
    "    if isinstance(optimized_json, str):\n",
    "        try:\n",
    "            optimized_dict = json.loads(optimized_json)\n",
    "        except json.JSONDecodeError:\n",
    "            optimized_dict = {}\n",
    "    elif isinstance(optimized_json, dict):\n",
    "        optimized_dict = optimized_json\n",
    "    else:\n",
    "        optimized_dict = {}\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"optimized_resume_json\": optimized_dict,\n",
    "            \"optimized_resume_text\": example.get(\"optimized_resume_text\", \"\"),\n",
    "            \"ats_score_original\": int(example.get(\"ats_score_original\", 0) or 0),\n",
    "            \"ats_score_regenerated\": int(example.get(\"ats_score_regenerated\", 0) or 0),\n",
    "            \"improvement\": int(example.get(\"improvement\", 0) or 0),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "    )\n",
    "\n",
    "def preprocess(\n",
    "    example,\n",
    "    _prompt_template=PROMPT_TEMPLATE,\n",
    "    _target_builder=_build_target_payload,\n",
    "):\n",
    "    prompt = _prompt_template.format(\n",
    "        resume_text=example.get(\"resume_text\", \"\"),\n",
    "        job_description=example.get(\"job_description\", \"\"),\n",
    "    ).strip()\n",
    "    target_json = _target_builder(example)\n",
    "    example[\"text\"] = f\"{prompt}\\n{target_json}\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8af44b",
   "metadata": {},
   "source": [
    "## Section 3.2 — Apply Preprocessing\n",
    "Map the preprocessing function across the dataset to create the `text` field used for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07fd6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/912 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 912/912 [00:04<00:00, 189.96 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 912/912 [00:04<00:00, 189.96 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 196/196 [00:04<00:00, 44.64 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 196/196 [00:04<00:00, 44.64 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 196/196 [00:04<00:00, 44.76 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 196/196 [00:04<00:00, 44.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['resume_text', 'job_description', 'optimized_resume_text', 'optimized_resume_json', 'ats_score_original', 'ats_score_regenerated', 'improvement', 'text'],\n",
       "        num_rows: 912\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['resume_text', 'job_description', 'optimized_resume_text', 'optimized_resume_json', 'ats_score_original', 'ats_score_regenerated', 'improvement', 'text'],\n",
       "        num_rows: 196\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['resume_text', 'job_description', 'optimized_resume_text', 'optimized_resume_json', 'ats_score_original', 'ats_score_regenerated', 'improvement', 'text'],\n",
       "        num_rows: 196\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = raw_dataset.map(\n",
    "    preprocess,\n",
    "    num_proc=4,\n",
    ")\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7335f6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert resume optimization and ATS analysis engine.\\n\\nTask:\\nGiven a candidate\\'s raw resume text and a job description, you must:\\n1) Rewrite the resume so it is strongly aligned to the job description.\\n2) Produce a structured JSON version of the optimized resume using the schema shown below.\\n3) Assign ATS-style match scores:\\n   - ats_score_original\\n   - ats_score_regenerated\\n4) Compute improvement = ats_score_regenerated - ats_score_original\\n\\nOutput rules:\\n- Return ONLY a single valid JSON object.\\n- Use EXACTLY these keys:\\n  \"optimized_resume_json\",\\n  \"optimized_resume_text\",\\n  \"ats_score_original\",\\n  \"ats_score_regenerated\",\\n  \"improvement\"\\n\\nResume text:\\n<<<RESUME>>>\\nOver 8 years of Software Testing experience, with expertise in Manual as well as Automated Testing.\\nExtensive experience in working with different domains such as Banking, financial, Travel, Hotels/Hospitality, Insurance, Airlines, Telecom, Publishing, Oil & Gas, Automobiles etc.\\nExperience working various types of financial applications for Trading regarding Mutual Funds, Securities, Stocks, Bonds, CDs, Money Market.\\nGood Understanding of different phase of Trade Lifecycle such as Account Set-up, Trade Order Generation, and Trade Order Execution, Order Settlement etc\\nExperience working with application that involves different securities such as Stocks, Bonds, CD, Money Market\\nStrong subject matter understanding in Risk Management, Derivatives, Fixed Income Assets, Mutual Funds, Hedge Funds, Portfolio Analysis and Investment Banking.\\nExperienced in developing Test Plans and Test Cases for different types of software testing.\\nGood experience in online banking, retail banking, mobile banking, Credit Card, Risk management, Credit Risk/Market Risk\\nProficient in Mostly Manual and some Automated Testing Tools.\\nExtensive knowledge of SDLC (Software Development Life Cycle) methodologies.\\nExperienced in testing various software applications including mobile (iOS, Android) applications.\\nExtensive testing experience on ANDROID, iOS and REX operating system Mobile Applications using Manual and Automated techniques.\\nExperience with installation, configuration of the Android SDK and ADT Plug-in for Eclipse IDE and setting up of the Android Framework Test Environment.\\nExperienced on testing Mobile apps on iOS3/4 and Android Platform.\\nExperience using Android DDMS features such as Log cat, Screen Capture.\\nSolid expertise in mobile testing using variety of Android devices.\\nGood working knowledge of validating backend database data and ETL process workflow by writing T-SQL queries in Oracle.\\nGood Knowledge of navigational flows for web based and mobile based applications as well as DVR\\'s.\\nExperienced in various types of testing including Sanity Testing, Smoke Testing, Functionality Testing, Performance Testing, Volume Testing, Unit Testing, Integrated Testing, System Testing, Positive and Negative Testing, Security Testing, Stress Testing, Load Testing and Regression Testing of Stand alone, Web Based Client-Server applications and Mainframe applications.\\nDevelop test cases (both positive and negative testing, boundary conditions) and use cases based on technical documentation and feature description.\\nExperience in working with different modules of ALM/Quality Center such as Requirements, BPT, Test Plan, Test Lab and Defects.\\nExperience in working with Business Analysts to understand the business critical functions and workflows define in BPT module of ALM/Quality Center.\\nPerformed Back End Testing by executing SQL queries.\\nInvolved in maintaining Test Matrix and Traceability Matrix and performed Gap Analysis\\nPerformed User Acceptance Testing (UAT) manually.\\nParticipated in design walkthroughs and verified QC Test Scripts and results.\\nTracked and reported on test execution.\\nCreate new, and modify existing SQL queries for use in the data integrity testing of the data warehouse application\\'s ETL process\\nTechnical Proficiency\\nTesting Tools: Quick Test Pro, WinRunner, SOAPUI, RFT,\\nTest Mgmt Tools: ALM/Quality Center, TFS, Microsoft Test Manager, VSTS, Microsoft Visual Studio Test Management\\nRequirement Mgmt Tools: Caliber RM, Test Director, ALM/Quality Center, JIRA, Team Track\\nScripting Languages: C, C++, TSL, VB, SQL, PL/SQL\\nProject Methodologies: SDLC, Agile, Waterfall, Spiral, RUP\\nServers: MS IIS, Apache, Web logic\\nDatabase: MS SQL Server, MS Access, Oracle\\nOther Tools: SharePoint, MS Excel, MS Word, MS PowerPoint, MS Visio, MS Project\\nPlatforms: Windows XP/ NT/ 2000/98/2007, UNIX, Mainframe\\nPROFESSIONAL EXPERIENCE\\nSr. QA Analyst\\nTD Bank - Jersey City, NJ\\t06/2020 to Present\\nThe consumer business of TD Bank includes Credit card, Small business, Home finance, Auto and education finance, Insurances. The commercial banking businesses include Middle Market, Corporate, Commercial Real Estate, Business Credit, and Equipment Leasing. Objective of this project is to facilitate an online banking customer with Bill Payment. It includes setting up the Payees\\' information and processing one time or recurring payments on the scheduled due date.\\nResponsibilities:\\nCreated and maintained operational certification test plan, test scripts and test sets.\\nEstablished testing strategy and plans.\\nCoordinated the creation of test scripts and test sets for all phases of testing.\\nEnsured test cases are traceable and cover all requirements.\\nOversee the execution of all testing, providing direction to all participants.\\nConducted Functional, Regression, System integration, Back End, System testing, Cross browser and Web Service/Web Methods testing.\\nInstalled the UFT/QTP for automating test cases and developed user defined functions to support Automated Scripts, used Data Driven Testing and Data Base accessing techniques to support the Scripts\\nPrepared automated scripts for Maintenance and Regression testing using UFT/QTP.\\nInvolved in Project Planning, Requirement Review, Defect Triage and Swing Calls.\\nCreated and maintained Cross browser library for Cross browser testing.\\nGenerated Automation Scripts from Scratch for UFT/QTP.\\nPerform Device Testing on Various Cellular and Tablet device for IOS, android, and Blackberry.\\nConducted Smart Phone validation (Android, iPhone, iPad and Black Berry)\\nInstalled and used \"Emulator\" to perform mobile test execution for Iphone, HTC droid, Samsung android, and Dell Streak\\nTested and coordinated mobile applications in both Native and Web environments and validated the mobile application functionality on physical smart phone hardware devices and virtual device emulators like perfecto mobile and Simulators like: Android SDK, IPhone SDK.\\nExecuted test cases through ALM/Quality Center and reported defects in ALM/Quality Center.\\nDeveloped and executed test cases test Scenarios and followed-up defects using ALM/Quality Center.\\nTracked defects using ALM/Quality Center.\\nInteracted with the developers to report and using ALM/Quality Center.\\nCommunicated status to Program Manager for multiple projects.\\nParticipated in the identifying and reporting of risks and mitigation plans.\\nCreate final test report for the operational certification.\\nHands on experience as QC Admin both server and client side.\\nEnsured QA methodologies are followed for all phases.\\nEnsured all requirements are met, through population of a trace matrix.\\nRan test execution war room calls, which includes monitoring of the test execution progress, defect resolution, issue management.\\nCreate status reports showing test execution progress, defect metrics.\\nTrain and Mentor users/testers on process and tools.\\nFew moths of hands on experience in creating site watch monitors using Site Scope (HP/Mercury Product).\\nEnvironments: HP ALM/Quality Center, SOAP UI, UFT/QTP, RFT, Clear Quest , Toad, SQL Navigator, MS Suite, SOAP Client, In house application to test Web services, Windows 95/98/2000/NT/XP/MAC, UNIX, Linux and MS-DOS, HTML, Emulator, perfecto mobile, MS Excel, VB Script, ALM/Quality Center, iOS, Android, Apple, Blackberry\\nAT&T - Dallas, TX\\t01/2018 – 02/2020\\nSr. QA Engineer\\nAgile Methodology, worked on a project for GRID MYATT wireless dashboard, Customer snapshot as GRID test design/testing in various project in AT&T mobility- worked with CST and data testing team\\nResponsibilities:\\nExperience with Layer 7 SME in customer snapshot project to work with all sources and get the data to create user stories for micro services and All API\\'s.\\nWorked with Oracle CCBS (Customer care and billing system) and integrate with different external systems\\nExperience to validate data using Customer care and billing to analyze with data and business requirements\\nWorked with different customer care and billing team and understand the scenario and create our test cases.\\nWork with client business and gather business requirement and validate the scenarios in Customer care and billing system.\\nExperience with IBM Rational integration testing to validate record messages between client and server\\nExperience with Rational Integration and analyze text message and server message\\nCreate and validate functional testing using Rational Integration tool.\\nExperience and strong knowledge on espresso to create test cases and using espresso scripts at Testdriod cloud and validate andriod tests\\nExperience with AS400 PBM to create test cases and execute test case and validate the scenarios.\\nExperience on UI Automator and create framework and provide API to build UI Tests and perform the operations in many projects\\nExperience in UI Automator API\\'s and viewer to run multiple scenarios and generate UI Automator tests.\\nExperience in Manhattan order management to validate customer details and overall inventory\\nInvolved with Manhattan order management and get customers full view and validate full network inventory and update entire customer order life cycle in many projects.\\nExperience with OSS/BSS Telecom and unix, linux Testing experience in AT&T Mobility projects\\nExperience with CABS (Carrier Access Billing System) to validate bill communication service providers for their customer\\'s access and network scenario\\'s update.\\nExperience with Protractor and develop framework using angular JS and web server manager\\nExperience with Trunks Integrated Record Keeping System (TIRKS) Operations Support Systems (OSS\\'s) to support Total Network Provisioning involved in projects with the planning, inventorying, assigning, provisioning, monitoring of the telephone networks.\\nExperience in Groovy, Cucumber to develop scripts and validate the scenarios as per business requirements.\\nGood Understand of Groovy, Cucumber, Protractor used to run tests using Angular JS with Spec.js and Config.js\\nGood experience with Protractor version and web service manager while working with protractor angular JS.\\nExperience and good understand of xUnit frameworks and used with test runner for executing program\\nNative events and browser specific drives used while writing tests in Protractor\\nMet preset customer goals through eCommerce sale maximization.\\nExperience in AEM (Adobe Experience Manager) worked with Content Repository\\nWorked with AEM and knowledge of content repository and use Restful Apache sling framework.\\nWorked with TFS Tools for Agile systems and write our tasks in TFS and validate our scripts.\\nPerform responsibilities of developing test scripts by using SOA test tools\\nGood understand of @ignore method in Junit with not execute test cases.\\nResponsible for estimating required resource and components for SOA testing\\nAssigned responsibilities of supporting Enterprise Service Bus (ESB) and Service Oriented Architecture (SOA) tier\\nProficient in Functional Testing tool Quick Test Professional (QTP/UFT) and the various frameworks in QTP/UFT.\\nExperienced in giving training to the functional testers to execute Test Cases using QTP/UFT as part of Regression Testing.\\nStrong knowledge of SOUPUI testing and Skilled with creating automated web services testing processes\\nExperience with AS400, Mainframe IBM to implement many projects\\nExperience with testing knowledge using tools RPGIII, RPG400, RPGILE, RPG-Free, CL, Screen Design Aid (SDA), DDS, AS/400 Utilities, MYSQL, and Sequel for AS400/Iseries.\\nGood knowledge with AS/400 Utilities, RPG400 and Screen design aid\\nExperience with Native, App, Hybrid application testing\\nExperience on HP UFT/QTP with Descriptive programming using Parametrization value.\\nWorked with DevOps Tools like Jenkins, Shell script, Chef, Puppet to develop automation and managing servers\\nExperience with Visual Studio .net framework C# VB.NET and validate the codes.\\nExperience on Robotium and Espresso for Mobile automation testing\\nWorked with Salesforce Dashboard / Reports with different reports like tabular, Matrix, Summary.\\nDesigned and tested many applications using Restful API Modules\\nTested and documented REST/HTTP APIs, including JSON data formats and API versioning strategy.\\nTest Development experience on BI/DW to validate data and generate reports\\nExperience on Test Data Management (TDM) involved with large number functional testing and performance testing\\nWorked with all test phases in IST, E2E, UAT and identify areas of risk.\\nGood Knowledge and experience in testing on AWS Cloud Environment.\\nExperience on Robot framework using Python to implement automation testing\\nExperience on Javaspock, XML, JSON and API testing to validate entire mapping documents also use Splunk and UNIX to validate columns as per our mapping sheet.\\nExperience with unit test frameworks with N Unit and X Unit\\nExperience on provisioning testing background in EBM, Customer snapshot projects.\\nExperience with IVR, ICOMS Billing system and ALM\\nExperience in CSG and ICOMS billing systems, OSM, UIM, IVR, switches.\\nVery Good Knowledge of CSG (Cable Service Group) and involved in payment processing system\\nExperience on main frame and load batch jobs and validate the result\\nExperience on DWBI Testing and worked on ETL with Informatica and Ab initio\\nUsage of Ruby, cucumber, Perl in many projects to automate test cases and validate\\nKnowledge of data prover in Test NG used for parameter for multiple testing\\nExperience of run width and parameter annotations used in Junit for parameterized Test.\\nUsing Jenkins to keep a track of version control system and to initiate ad monitor a build system if changes occur in many projects.\\nSuite test, ignore test, exception test, timeout used in both Test NG and Junit\\nWorked with CA LISA automation tool to validate multiple test cases.\\nExperience and working knowledge of X code and Android in customer projects\\nDeveloped SQL, PL/SQL Procedures, Packages, Functions, DDL and DML Queries.\\nRun different type of SQL Queries, update PLSQL with Functional and Procedures in many projects\\nAnalytical and problem solving skills, applied to Big Data domain proven and understanding with Hadoop, HBase, Hive, Pig\\nFamiliarity with data loading tools like Flume, Sqoop and data validation\\nWell working knowledge in portal testing, GUI Testing, Salesforce in many projects\\nWorking with Agile methodology for API Validation successfully done in various projects\\nWorking experience in Dynamic Object Oriented Requirements System (DOORS)\\nExperience on Hyperion to generate summarized reports and multi-dimensional reports\\nand QC ALM with good knowledge in QA/DHF documentation and maintenance of DHF\\nExperience on ETL testing and validation with many projects and working with Salesforce reports\\nExperience in CST testing using GRID environment for Myatt, EBM, HALO projects.\\nGRID Test experience on eclipse with java coding and validate TELEGENCE, ROME, UDAS, ECDW DATA, ENABLER HDS-IMEI, DTI, CCR-R, CCR-L with end to end validation and experience on Cassandra backend\\nWorking experience on power shell and Bash in many projects\\nGood working Experience in Mobile testing (Andiod testing) using see test, eggplant, appium tools\\nSee test fully integrated with HP Quality center and JIRA\\nExperience with SOA Test and Soap UI pro Test in many projects\\nExperience in Functional testing, regression testing using Soap UI PRO\\nWorked with Data testing team(TDSS) and generate reports using pivot tables\\nCapital One - Richmond, VA\\t09/2015 – 12/2017\\nSr QA\\nWorked on developing E-commerce application- CANADA Secure Card and Campaign Management IT.\\nResponsibilities:\\nWell experience in CRM (customer Relationship Management) for Customer order\\nInvolved many projects using Soap UI Pro for functional and regression testing\\nWell experience in Mortgage banking with capital one bank.\\nExperience with E-commerce and create test case and validate the scenarios.\\nWell knowledge about banking loans with the customer credit line.\\nWorking with mortgage loans projects and client requirements\\nWorking with Canada Secured Card systems with various projects using Unix/Control-M/Oracle/Teradata environment. Support L3 Level requirements and analyze alert messages with quickly solve the errors and failure of daily/weekly/monthly jobs.\\nPerform mobile test execution, regression testing, functional system testing sanitary testing smoke testing\\nExperience with testing techniques, phases and tools such as HP ALM\\nExperience with PL/SQL, CQLSH, SQL testing\\nWorked with UNICA Campaign management team (CMIT) and involved various projects using more than 100 Jobs in campaign schedule management.\\nRestore client data from various external sources (FTP) to our file system\\nLarge volume data conversion, data cleansing, production report generation, and upholding scheduled data delivery standards\\nPerform Data validation and massaging to ensure accuracy and quality of data\\nExperience in Dashboard production and Responsible for design, development of a diversity Dashboard Responsible for data access and delivery technologies linking data to the Dashboard output and create management level output and views\\nWorking experience in API, Data Migration, Metadata with SQL Developing.\\nWorking experience in CTRL-M Schedule Changes with different jobs.\\nWorking with UNIX environment and writing queries in Oracle/Teradata for business requirement to satisfy Data ware house concepts.\\nKnowledge of Power Builder and identify the transactions using PB connecting various testing/Developing & Production servers.\\nRequirement gathering, design, development of coding, testing and involved in Roll out and Post Production for each project.\\nAnalysis of the specifications provided by the clients.\\nInvolved in world permission scan UNIX coding and complete development and roll out / post production.\\nImplemented Multiple signal file fix project in Unix environment to run oldest file first and other files waiting for rerun the jobs\\nWorked with various system interfaces to gather requirements for migration and implementation.\\nExtensively involved in creating audit reports by extracting from the data warehouse.\\nWell knowledge of ftp in UNIX environment using put/get of export import data from various UNIX servers.\\nLoaded and extracted data into Teradata using Fast Load, BTEQ, Fast Export, Multi Load, and Korn shell scripts.\\nCoding Unix BTEQ Wrappers for loading and extracting data using BTEQ SQL of TERADATA\\nWrite Unix shell scripts to validate, format and execute the SQL\\'s on UNIX environment\\nExperience in Control-M Jobs with knowledge of late alerts and identify the failure of jobs using Unix scripts/ SQL Queries\\nMacys.com, Atlanta GA       \\t08/2013- 08/2015\\nSr. QA Analyst/Mobile tester\\nClient is a major department Retail chain store of garments and fashion products. As a QA Analyst with Ecommerce team, the project is to test the Order Management System was part of the e-commerce application that was being developed for company’s retail website. Worked on the shopping cart application, the Online Gift card and the B2B exchange of data & online transaction processing & electronic funds transfer. The customer’s orders an item as a registered customer or guest, the customer entered the gift certificate number which was then reflected on the financials showing some savings in the place order page before applying the credit card.\\nWorked with SMEs for SUT and provided an overview and knowledge regarding various modules of application to test team members.\\nCreated test plan, test cases and test procedures to cover various test scenarios to cover all areas of applications and to conduct different types of testing\\nWork in Agile, scrum, and sprint environment in order to change the requirements and features set.\\nPerformed SIT and UAT test execution activities throughout STLC\\nDefect tracking\\xa0Microsoft\\xa0Visual\\xa0Studio\\xa02010 (TFS)\\nUsing\\xa0TFS\\xa0for Test case management, and Defect Management purposes.\\nPrepared automated scripts for Maintenance and Regression testing using QTP.\\nAutomated test scenarios for GUI, Functionality and performed data driven testing on the application inserting different data in excel sheet using QTP.\\nCreated and maintained detailed test cases to perform various types of testing in Test Plan module of MTM.\\nCreated and executed automated (Coded UI) scripts in Visual Studio and in TFS\\xa0Test\\xa0Manager\\nPublishing the application in production for all platforms (Android, BlackBerry).\\nPerformed Regression, functional testing of Retail and eCommerce application\\nGathered requirements for the integration of POS system with the supply chain system\\nSupported retail business processes and functionalities including POS application and systems; merchandise processes, such as allocation, goods receiving, store transfers, return to vendor.\\nCreated and executed automated (Coded UI) scripts in Visual Studio and in TFS\\xa0Test\\xa0Manager\\nDocumented Defects found during test on the Microsoft\\xa0Team\\xa0Foundation\\xa0Server (TFS)\\xa0and communicated recorded problems to the responsible development personnel.\\nDocumented and reported defects within established process and tracking systems using MTM.\\nInvolved performing end to end testing of Ecommerce applications\\nDefine and implement black-box, white box, grey-box and automated testing suites for eCommerce Web application and Order Management Systems\\nI was exclusively working on “Warehouse Management” which is a web-based logistics system that provides an easily deployable method for receiving, storing and shipping inventory through one or more remote facilities\\nInvolved in testing of both Inbound and outbound processes of Red Prairie WMS\\nResponsible for functional testing of Receiving, Picking, Shipping, Order Fulfillment, Reclamations, Cycle Counts, Inventory Control, Invoice Counting, Reports\\nPerformed Regression Testing by generating QTP scripts based on the business process.\\nPerformed Data Driven Testing using Parameterization in QTP to test the functionality of the application with different sets of data.\\nExperience in conducting various types of System and Integration testing for an E-commerce web based application such as Security(to validate Username/Password), Account Creation process, different workflows, e-mail testing for each stage of workflows, Role level access based on the access grants, navigation testing to check the integrity between different screens within the application etc.\\nValidated various canned and ad-hoc BO reports related to the application.\\nBuilt RTM (Requirement Traceability Matrix) to capture all requirements in the testing process.\\nEnvironments: Oracle, Java, .net, Team Foundation Server, MTM (Microsoft Test Manager), TFS, Visual Studio, Quick Test Pro, UNIX, Share Point, UNIX, POS Registers, Emulator, perfecto mobile, MS Excel, VB Script, iOS, Android, Apple, Blackberry\\nUnion Bank of California (UBOC) - Los Angeles, CA\\t02/2011 – 07/2013\\nTest Coordinator / Mobile Tester\\nBanking Division provides a variety of products and services to individual that include mortgage for home purchase, home equity loans, re-financing, reverse mortgage, Payment Power and specialized mortgage loans.\\nRetail and Private Banking on Smart Phones (iOS – iPad & iPhone (Primary focus), Android, Blackberry & windows)\\nThis Project aims at providing all the retail & private banking features over the smart phones (iPhone, iPad, Android, BlackBerry) to consumers along with the advanced features (Quick Deposit-Check deposit over Phone and Quick Pay along with Account Summary, Transfer Funds, Bill Pay, Wire Transfers, ePay (Credit Cards), Alerts functions and ATM/Branch Locator using native maps.\\nParticipating in the requirements analysis and design meetings with DEV, BIZ team for Review and facilitate in requirement freezing.\\nPlanning all the testing activities (Test Planning, Test Estimation and scheduling of QA activities, Test Strategy, Test Case Design, Requirements Traceability matrix, Review, execution and Reports)\\nTested the UI of the end product on mobile, PC , Android and Apple platforms to ensure that language translation and currency conversions were displaying as required\\nRequested different versions of iOS, Android and Blackberry mobile devices\\nWorked on a E-Commerce Web-based application accessed by potential customers for all the different offerings\\nWorked with java based applications  and C++  simultaneously: iOS and Android\\nPlanning and execution of Installation Testing, Smoke Testing, Functional Testing, Regression Testing, Integration Testing, System Testing, usability testing, Compatibility Testing, Performance Testing and security testing.\\nExecuting the test cases (manual and automated) and reporting the bugs in Mingle and QC/ALM.\\nMentoring and Monitoring the QA Team, troubleshooting any technical problems.\\nPerformed browser compatibility and configuration testing on various operating systems such as Blackberry OS, Android OS, iOS etc.\\nCollaborating with development team and project managers in test execution and communicating all the issues.\\nGenerating project status reports/metrics and updating project plans and Tracking them.\\nCoordinating and leading the reviews and inspections and follow up on ongoing test activities.\\nConducting QA status/Defect triage conference calls with US clients and reporting the status to QA Manager.\\nProviding regular feedback/status on development and projects to Project Manager.\\nInvolving in interviewing the new candidates for the project and providing training and mentoring support on all QA activities.\\nPublishing the application in production for all platforms (Android, BlackBerry).\\nEnvironment:\\xa0 Oracle 10G, Java, Asp.net, ALM/Quality Center, Quick Test Pro, HTML, UNIX, Web logic, Emulator like Device Anywhere, perfecto mobile, Android SDK, Windows Mobile SDK, IOS, Blackberry OS, Android OS\\nFarmers Insurance, Orange, CA\\t02/2010 – 01/ 2011\\nSr QA Engineer / UAT Tester\\nFarmers Data warehouse System\\nFarmers Insurance Data is the huge collection of data coming from Mainframe, DB2, Oracle, Flat Files and external feeds. Requirement of this project is to create a huge data warehouse system which contains the major data entities like Policy, Customer, Quotes, and Claims. Farmers Insurance has acquired 21st Century into their business so they want to collect the data who took the quote from the 21st Century and didn\\'t took policy identify them and provide a quote if they are non-existing customers.\\nResponsible for coordinating the manual QC and automation QTP to end-to-end and UAT testing efforts for new release testing.\\nResponsible for compliance to standards and tools for unit, component and system integration level test results, defect management, and test status reporting.\\nReport and maintain product release statistics\\nEstablish regression execution patterns and schedule.\\nWorks with offshore team and provides effective solutions to work with offshore teams to enable multiple shifts for regression testing.\\nValidating the load process of ETL to make sure the target tables are populated according to the data mapping provided that satisfies the transformation rules\\nCreate effective solutions for regression Test artifact reviews\\nPlans, manages strategic and tactical aspects of regression and manages multiple Releases effectively.\\nFacilitate team status/checkpoint meetings as necessary\\nScheduled various ETL batch processes using Autosys and testing stored procedures\\nTested the ETL Informatica mappings and other ETL Processes (Data Warehouse Testing)\\nAssist Test Manager and Planner with resource forecasts and estimates\\nAssist in early life support of the Releases.\\nWorked closely with development and business team to understand requirements and business process and defined issue resolution with functional and technical teams.\\nTested ETL mappings to extract and load data from different databases such as Oracle, SQL Server, XML files and flat files and loaded them in to Oracle\\nInvolved in scheduling and participation of weekly status, requirement analysis, functional specification (functional and technical teams) reviews meetings.\\nInvolved in peer review and group reviews of the test scripts developed by test team as per the Coding standards to test asset quality. Decisions within defined to do policies and\\npractices.\\nEnsured on time test deliverables as per schedule by generating defect Status reports,\\ntest status reports at the end of each Functional, System and Integration test phases.\\nCreated and maintained System and Automation Test plans. Performed Functional,\\nNegative, Regression, System Integration testing.\\nInvolved in Coordinating testing team at offshore and onsite for test design, construct test\\nscenarios, and execute tests to plan accordingly.\\nUsed QC for designed and developed test plans, scripts, and tools using the detailed business requirements document provided by the work streams.\\nEnvironment: SQL, Agile Methodology, Internet Explorer 8.x, Oracle, J2EE , Power Point ,MS Project, Quality Center ,QTP,SQL and  E-commerce.\\nPremier Healthcare, Charlotte, NC \\t04/2009 - 01/2010\\nTest Engineer / SOA Tester/ Web Services Tester\\nPremier Healthcare is a non-profit organization. Premier collects data from participating hospitals and houses the nation\\'s largest detailed clinical and financial database, holding information on more than 130 million patient discharges. Web-based tools allow hospitals to compare their performance in specific areas to peers and best performers, find opportunities for improvement, and track the results of their efforts.Working on 837, 834 and 835 transactions, EDI in FACETS application.\\nWorking on BRD, FRD, TRD, SRD, RTM, Test Plan-overview, Test Scope(In-Scope& Out of Scope), Version, Test Approach/Strategy, Test Schedule, Test Cases/Test Scripts, Test Rounds, Test Traceability Matrix, Defect Management &Reporting, Entry & Exit Criteria, Environment, References.\\nBRD-Involved in writing Business Requirement Documents overview, scope, version, role and responsibilities, signoff approval\\nFRD-Involved in gathering requirement, one O one interview, group interview, questionnaire and survey, brain storm, interview to obtain idea, facilitated session, use case etc.\\nSDLC- Waterfall Methodology-Conceptualization, Requirements and Cost/Benefits, Software Requirements, Software Design, Programming, Testing, Maintenance.(Completed all phases of SDLC moved to next phase)\\nAgile Methodology-Heavily involved into. Iteration planning, Sprint, Scrum Meeting, Go Live Date\\nHybrid Waterfall -Agile we used SDLC Phases Planning, Requirement, Design, Development Documents from Waterfall Methodology. We used unit testing, Integration testing, System Testing and UAT testing from Agile methodology.\\nResponsible for full HIPAA compliance lifecycle from gap analysis, mapping, implementation and testing and processing of Medicaid Claims.\\nParticipated in the development of the Business Requirement Documents and functional requirements. In charge of preparing Test Cases based on BRD and FRD.\\nCreated test scenarios and tested HIPAA Transactions and Code Sets Standards such as EDI 837/834/835, 270/271, 276/277.\\nTesting claims or enrollment processing for medical/Pharmacy\\nHeld regular JAD meetings with the system architects, developers, database developers, quality testers during the entire project to assure that the critical as well as the minute details of the project were discussed and issues were resolved beforehand.\\nFor web service testing used Soap UI, Soap UI Pro tool, Analyzing of web services/API functional and design documents and prepared test cases based on User requirements, Involved in preparation of Test Plan, Prepared the Web Services/API test suite for positive and negative test cases using assertions, Performed Interoperability Testing when services are integrated to UI application, A WSDL document describes a web service.\\nPrepared Execution Matrix, Test Reports and Bug Reports\\nInvolved in web service REST Testing tools-Synchronous Web services and Asynchronous Web services.\\nResponsible for GAP analysis of ICD9-ICD10.\\nAuthored and executed Test cases for Claims and Customer Service Workflow.\\nEntering Claims and Customer Service Tasks into the FACETS.\\nResponsible for Back-End Testing Using SQL Commands.\\nDeveloped SQL queries, functions, stored procedures and triggers to perform the backend testing of the data\\nPerform functional, exception and scenario testing.\\nExtensive experience of Performance, Load, and Stress testing using Load Runner\\nLogged the errors, reported defects, determined repair priorities and tracked the defects until resolution using Quality Center.\\nExtensively used SQL statements to query the Oracle Database for Data Validation and Data Integrity.\\nExtensive knowledge of Test Matrix, Traceability Matrix.\\nQuality Center was used to create the test plan, store test cases and run the test sets.\\nProvides feedback concerning completeness and accuracy of AUT.\\nResponsible for performing System and integration testing for release.\\nEnvironment: Windows, Facets, Oracle, Share Point, SQL, Quality Center, MS Office, MS-Visio, UML, 508\\nCompliance Testing\\nGreat-West Life & Annuity Insurance Company, Greenwood Village, CO\\t07/2008 – 03/2009\\nTest Engineer\\nGreat-West is one of the largest Life & Annuity Insurance Company which provides individual Life and annuity products. Tested various Life & Annuity insurance applications. Tested the Under writing, Membership, Claims processing applications Worked as a QA Tester in testing the Web applications, Eligibility, Membership and Claims processing applications.\\nResponsible for  Post Conversion and POS (Point of Sale) to ensure data is mapped correctly according to system requirements\\nResponsible for coordinating the manual end-to-end, backend and UAT testing efforts for new release testing in quality center.\\nDocumented and tracked test scripts, test results, test analysis, and reported the defects using Quality Center and assisted the higher levels of management in analysis of risks and issues in project releases.\\nCreate Test scenarios, Test Plans and Test cases.\\nPrepared the Test Status Reports on daily and weekly basis.\\nPerformed different tests like Function, System, Regression and backend Database testing.\\nMaintained good communication with the Onsite and Offshore team members.\\nRun the SQL queries to fetch test data and backend (database) testing to validate the data using TOAD tool\\nMaintaining Traceability Matrix between the functional requirements and the test scripts developed and application theories and new concepts.\\nReport the defects to development team and re-verify the defects when it\\'s fixed, also need to update the status in HP Quality Center.\\nPerformed Test Case Review, Execution and walk troughs.\\nPrepared test plans, Design and preparation of test data scripts for testing POS and E-commerce application.\\nInteracted with the business users, prepared test plans, including testing overview, testing approach, testing strategy, roles, and responsibilities and Monitoring and reporting the performance and effectiveness of QA Activities, writing and reviewing and closing defects\\nEnvironment: TOAD, Windows XP, HP Quality Center, QTP, XML, Html, Java Script, SQL\\nThe Western Union Company, Union, CA               \\t         10/2007 – 06/2008\\nQA Analyst\\nWestern Union is a financial services and Communications Company based in the United States, Western Union has several divisions, with products such as person-to-person money transfer, money orders, business payments and commercial services.\\n\\nProject 1: Web and API Testing for western union site\\nResponsible for coordinating Scope of testing, testing approach, Entry/Exit Criteria, Design and preparation of test data, Defect management, test Reports and summary of test results Used BPT, Quality Center to performed Manual and Auto testing and ensure proper integration with other modules like credit card processing, Agile methodologies rely on face-to-face communication and collaboration with people working in pairs, ecommerce, POS, MM, integrate between business needs.\\nExecuted test case for all HR processes and services including iViews, pages, work sets and roles\\nDocumented Test cases corresponding to business Requirement document and other negative conditions.\\nInvolved in Developing Test Plans, Test Cases, Test Scripts and Test Data.\\nPerformed End-to-End testing manually.\\nResponsible for GUI Testing, API Testing, System Testing, Regression Testing and Acceptance Testing.\\nTested the backend database (Oracle/DB2) using SQL queries.\\nWorked with XML/SOAP UI (web services)\\nExtensively used Groovy scripting for Soap UI frame works in API testing.\\nDeveloped automation test scripts in Groovy Script for Smoke testing of the application using SOAP UI.\\nCommunicated defects encountered during regression test and follow up with developers until all issues were resolved\\nCreated the complex SQL queries to fetch data from the multiple tables based on the Test cases.\\nTested Westernunion CST tool\\nResponsible in providing regular test reports to the management.\\nReported the defects to the developers using Jira defect reporting tool.\\nInvestigate software defects and interact with developers to resolve technical issues.\\nResponsible for test data preparation for the system/integration testing, traceability matrix to ensure the test case coverage and defect logging.\\nParticipated in defect review meetings with the team members.\\nEnvironment:.NET, SQL, Agile Methodology, Web Logic,API, Internet Explorer 6.x, Oracle, PVCS, J2EE , TLG, Enabler, Power Point ,MS Project, Quality Center,WCF,ASP.NET, XML, Windows XP, UNIX,IVR and TOAD\\nUnion Bank - Jersey City, NJ\\t01/2007 - 09/2007\\nQA Analyst\\nProject was testing online credit card application. An individual can apply for the credit card online through the Website by entering the required details. Upon satisfaction of the conditions (i.e., income, credit rating) an instant credit card number, credit limit, expiration date, rewards and a welcome kit will be released to member through the internet and sent to the Email -ID and the credit card will be sent to the member by mail. Union bank\\'s mobile banking application helps the customers to access their accounts whenever and wherever they want. Through the mobile app. Customer can view accounts balances, transactions and pending payments, pay bills to payees within US and also find nearby ATMs and branches.\\nInvolved in complete software testing lifecycle (Requirement Analysis, Test Planning, Test Cases and Scripts Development, Test Execution, Test Reporting, Test Result Analysis, Defect tracking, Test Closure).\\nAnalyzed business requirement documents, functional requirement documents, Use cases, Use case diagrams.\\nDocumented test plan, including test strategies, test cases and schedules in Quality Center.\\nWorked closely with developers and business analysts to formulate test plans, test strategies and test cases in the pre-testing phase for Smoke testing, Functional testing, System testing, Regression testing and Data driven testing using QTP.\\nCustomized test cases and schedules for Credit Card Online application\\nQueried the Mainframe ETL Target DB2 database to verify the data consistency based on ETL Business Rules.\\nResponsible for Testing Web Services requests using SOAPUI\\nPerformed GUI Testing on web and mobile applications.\\nUsed data driven testing features to provide data values to the variables in a script.\\nPerformed manual testing and automation testing of the web application.\\nWrote extensive scripts to perform smoke tests for verification of weekly builds.\\nPerformed various types of testing like Smoke testing, Functional, Integration, System, Regression, Data driven testing, usability testing.\\nInvolved in testing mobile Banking Module for \"Find Locations\", \"check accounts\" and \"pay bills\" on iOS, Android and BlackBerry operating system.\\nPerformed compatibility testing, interruption testing, and network testing for stability of mobile app.\\nUsed TOAD for writing SQL queries for validating, retrieving and comparing the data.\\nDesigned and developed SQL queries to test back end.\\nPerformed cross-browser testing to verify if the application provides accurate information in different (IE, Firefox, Safari) browsers\\nMaintained Quality Center defect-tracking tool, including submitting and assigning defects to the application developers along with the release of defect aging reports.\\nVerified Tracked and Reported defects and classified based on Severity and Priority.\\nCollected and analyzed test metrics and subsequently submitted reports to track the status and progress of the testing effort.\\nInteracted with the Project Manger to provide estimates for different phases in the projects and also interacted with the client team to convey the progress\\nEnvironment: QTP, SOAP, Oracle, Quality Center, Mozilla, IE, Windows 7/XP, PVCS, Citrix, C#.Net, MS-Office, Java, J2EE, Servlet, ASP, Linux XML, MS-Project, Web sphere, Web Logic, VB script, TOAD\\nPMA Insurance Group, Blue Bell, PA\\t                                           12/2005 - 12/2006\\nQA Analyst/UAT Support\\nThe PMA Insurance Group a premier\\xa0property\\xa0and\\xa0casualty\\xa0insurance organization specializing in workers\\' compensation and integrated disability management, it offers workers compensation programs for businesses of all sizes and for a wide range of industries. It is first insurers to bring managed care to workers\\' compensation, helping clients control claims costs through accident prevention and by quickly and safely returning injured workers to their jobs by effectively managing the medical and indemnity component of a claim.\\nResponsible for coordinating the manual end-to-end and UAT testing efforts for new release testing, environment and calendar set-up, planning, and coordination with other ET/QC groups for all applications that interacted with CPC, Siebel v7.5, and POS applications.\\nFormulated detailed Test Plan, Test Cases and Testing Procedures, which included Test Cases/Scripts, capturing Test Results and capturing and resolving Test Anomalies.\\nResponsible for Manual Testing and Automation Testing.\\nPerformed configuration/ compatibility and user interface testing manually.\\nAutomated test scenarios for GUI, Functionality and performed data driven testing on the application inserting different data in excel sheet using QTP\\nHave experience designing and writing training manual templates, design formatting, project planning and writing technical documents.\\nExecuted automated test scripts as batch tests, analyzed the results and reported the bugs in Quality center\\nPerformed Data driven testing, designed Input/Out put check points to validate the data and develop effective Automated Scripts.\\nEnhanced QTP scripts by inserting Standard Checkpoints, XML Checkpoints, Database Checkpoints, Table Checkpoints and Page Checkpoints.\\nPerformed Business Process Testing using BPT module of Quality Center and QTP.\\nConducted GUI/Usability Testing and Black Box Testing according to specifications.\\nInvolved in writing complex SQL queries to check the data integrity\\nPerformed Smoke and Sanity testing.\\nPerformed Positive and Negative Testing\\nPerformed Security testing and Backend testing Manually.\\nParticipated in conducting System testing and End to End testing Manually.\\nExtensive UAT Testing Manually.\\nPerformed Usability and Integration testing Manually.\\nUsed Test Director for bug tracking and reporting, also followed up with development team to verify bug fixes, and update bug status.\\nDocumented bugs found out during the process of testing.\\nEnvironment: TestDirector, Quality center, QTP, WinRunner, UNIX, Oracle, HTML, TOAD, SharePoint, MS Virtual Rooms.\\nAMERISTAR INFORMATION NETWORK - Dallas, TX\\t01/2005 - 11/2005\\nQA Tester/Analyst – Consultant\\nThis project was an application for mortgage, home loan, ecommerce, billing and appraisal orders for Ameristar\\'s customers including HR systems and used AS400 for validate the   billing orders.\\nUsed Test Director for designed and developed test plans, scripts, and tools using the detailed business requirements document provided by the work streams. This included performance and regression testing where required.\\nUsed SQL Statements for manipulating the data.\\nWorked closely with lead and infrastructure work stream lead to ensure that all testing activities occurred in a timely fashion and in accordance with the level of quality required by the governing standards and business requirements.\\nResponsible for compliance to standards and tools for unit, component and system integration level test results, defect management, and test status reporting.\\nDeveloped automated LoadRunner scripts and defined test case scenarios.\\nUsed Test Director to execute the test plan, track execution against the plan during testing of employee pay roll, UNDERWRITING, processing and reporting, benefit tracking, direct deposit with the capability of using the customer\\'s bank of choice .\\nGathered test data and created test plan for all kinds of loads up to maximum of 500 virtual users.\\nUsed Load Runner for Created scenarios to perform automated load, regression and stress tests.\\nPreparation of test presentation on the existing UNDERWRITING application which details the test processes and testing technologies to be implemented.\\nPrepared defect reports.\\nEnvironment: Test Director, MS SQL, UNIX, Agile Methodology, Visio and Project 2003\\nEDUCATION\\nMaster of Computer Application\\nDuke University, North Carolina\\n<<<END_RESUME>>>\\n\\nJob description:\\n<<<JOB_DESCRIPTION>>>\\nSIGN INJOIN NOWCreative Services Producer jobs in Springfield, MOOverviewCompanyThis job has closed.APPLY to similar jobsNexstar Media Group, Inc. · 1 week agoCreative Services ProducerSpringfield, MOFull-timeHybridEntry Level$18/hr - $18/hr1+ years expNexstar Media Group, Inc. is seeking a Creative Services Producer responsible for producing a variety of promotional spots and commercial content. The role involves creative scripting, shooting, and editing, as well as collaborating with the creative team to deliver compelling videos and digital content.BroadcastingInformation ServicesMedia and EntertainmentNewsPublishingSocial MediaResponsibilitiesTagging Network promotional spotsDevelopment of cutting-edge commercial and promotion spots from concept to completionProducing a wide variety of Ozarks First promotional spotsProducing commercial sales Promotional spotsProducing community spotsProducing news topicalsProducing branding and image spotsProducing longer-form lifestyle show segmentsAssisting at station sponsored eventsDeveloping concepts and writing scripts as assignedProviding input regarding equipment and software needsPerforming any duties as assigned by the Creative Services Director or the Director of MarketingCreatively writing, shooting, and editing content to tell compelling storiesCollaborating with creative team and project stakeholders to produce videos and digital content that are on-brand and drive resultsCoordinating asset and information exchange and taking meeting notesMaintaining a variety of projects and working with varying production styles while employing strong client service skillsCoordinating production, including working with Director of Marketing, clients, account executives, agencies, as well as planning location(s), talent, music, voice-over and reserving any other technical pieces (camera’s, vehicles, etc.) needed for projectManaging video equipment and software needsExtensive shooting and scripting of both short and long form spots from concept to completionTagging Network spots with local logo / timesGraphic design back-up as neededInterpreting the client\\'s business needs; developing design briefs by gathering information and data to clarify design issuesPerforming other duties, as directed by management, including logging station promosQualificationAdobe Creative SuiteVideo productionScriptingMotion graphicsClient service skillsPost-production processGraphic designTeam collaborationProject managementCommunication skillsRequiredMust be proficient with Adobe Creative Suite. Specifically with Premiere, Photoshop, After Effects and Illustrator.Creatively write, shoot, and edit content to tell compelling storiesCollaborate with creative team and project stakeholders to produce videos and digital content that are on-brand and drive resultsCoordinate asset and information exchange take meeting notesMaintain a variety of projects and work with varying production styles while employing strong client service skillsCoordinate production, including working with Director of Marketing, clients, account executives, agencies, as well as planning location(s), talent, music, voice-over and reserving any other technical pieces (camera’s, vehicles, etc.) needed for projectManage video equipment and software needsExtensive shooting and scripting of both short and long form spots from concept to completionTag Network spots with local logo / timesGraphic Design back-up as neededInterpreting the client\\'s business needs; developing design briefs by gathering information and data to clarify design issuesPerform other duties, as directed by management, including logging station promosMust possess strong writing abilities that can clearly and concisely communicate client messages in both short and long form contentMinimum 1-2 years commercial and/or promotion experience, or college degree preferredAdvanced knowledge of the post-production process, including media management and encoding video to various formatsBroad understanding of marketing and designAdvanced knowledge of High-definition (HD) cameras and videography, motion graphics and proper lighting techniquesMotion-graphics skillsExhibit advanced editing expertiseA passion to stay up to date on production trends and continually strive to push boundariesSoftware skills required include Adobe CC Suite production package including Premiere Pro, After Effects, Photoshop, Illustrator, Audition and Media EncoderPosition will require day-to-day multi-tasking, as well as managing multiple projects simultaneously with frequent change in direction and priorityMust be able to work independently or as part of a teamOccasional weekends, evenings, and holiday shifts will be requiredMust maintain a valid driver’s license and good driving record as some travel may be required for commercial shoots and production meetings with advertisers of the stationPossess a strong understanding of fundamental design, a strong sense of color, typography, and compositionMeet all deadlines, fulfill scheduling commitments, and consistently achieve quick turnarounds in a fast-paced, rapidly changing environmentPreferredMinimum 1-2 years commercial and/or promotion experience, or college degree preferredBenefitsMedicalDentalVisionVacation401K matchPaid Parental LeaveCompanyNexstar Media Group, Inc.Glassdoor2.8Nexstar Media Group, Inc.Founded in 2000Irving, Texas, USA10001+ employeeshttp://www.nexstar.tvFundingCurrent StagePublic CompanyTotal Funding$2.5M2017-09-06Post Ipo Equity· $2.5M2003-12-05IPOLeadership TeamBill SammonSVPJerry WalshSenior Vice President of Local Content DevelopmentRecent NewsTV TechnologyNexstar Deploys New Transmitter Tower for WDVM-TV2025-10-07Boston GlobeWPRI’s longtime general manager departs to lead top station in Washington2025-10-06Business Insider\\'SNL\\' returns to a late-night world still finding its footing in Trump\\'s 2nd term2025-10-05Company data provided by crunchbaseBoost Your Interview ChancesImprove Resume Match ScoreFREE?Your Score8.9Top ApplicantsMust-Have Skills for This RoleAdobe Creative SuiteVideo productionScriptingMotion graphicsClient service skillsOptimize my ResumeGet Referral Via linkedIn FREE3× Higher Response via Email OutreachJJack S.Creative Services DirectorFFabiola F.Producer, Content LicensingDraft Message to ConnectApply Faster with Autofill PluginFREEApply With AutofillWelcome to JobrightSign in with GoogleSign in with Google. Opens in new tabSign up with GoogleORI want to receive updates from Jobright about latest job offersSIGN UPAlready a member?\\xa0  Sign in now By continuing, you agree to the Jobright\\xa0Terms of Serviceand the\\xa0Privacy PolicyWelcome to JobrightSign in with GoogleSign in with Google. Opens in new tabSign in with GoogleORSIGN INNot a member?\\xa0  Sign up now\\n<<<END_JOB_DESCRIPTION>>>\\n{\"optimized_resume_json\": {\"personal_information\": {\"name\": \"Unknown\", \"email\": \"\", \"phone\": \"\", \"location\": \"\"}, \"summary\": \"Professional with relevant experience\", \"experiences\": [], \"education\": [], \"skills\": [], \"projects\": [], \"certifications\": [], \"awards\": [], \"languages\": []}, \"optimized_resume_text\": \"Unknown\\\\n\\\\nSUMMARY:\\\\nProfessional with relevant experience\", \"ats_score_original\": 20, \"ats_score_regenerated\": 20, \"improvement\": 0}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53bfa87",
   "metadata": {},
   "source": [
    "# Section 4 — Load Model with QLoRA\n",
    "Load Qwen2.5-3B with BitsAndBytes 4-bit quantization and attach a LoRA adapter to the attention and MLP projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d765826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8fc6b",
   "metadata": {},
   "source": [
    "# Section 5 — QLoRA Fine-Tuning with TRL\n",
    "Train with TRL's `SFTTrainer` for three epochs using cosine learning-rate scheduling and 2k token context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfaca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n",
      "Adding EOS to train dataset:   0%|          | 0/912 [00:00<?, ? examples/s]You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n",
      "Adding EOS to train dataset: 100%|██████████| 912/912 [00:00<00:00, 3274.68 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 912/912 [00:00<00:00, 3274.68 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 912/912 [00:11<00:00, 78.57 examples/s] \n",
      "Tokenizing train dataset: 100%|██████████| 912/912 [00:11<00:00, 78.57 examples/s]\n",
      "Packing train dataset: 100%|██████████| 912/912 [00:00<00:00, 26057.43 examples/s]\n",
      "Packing train dataset: 100%|██████████| 912/912 [00:00<00:00, 26057.43 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 196/196 [00:00<00:00, 3698.10 examples/s]\n",
      "Tokenizing eval dataset:   0%|          | 0/196 [00:00<?, ? examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 196/196 [00:02<00:00, 80.39 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 196/196 [00:02<00:00, 80.39 examples/s]\n",
      "Packing eval dataset: 100%|██████████| 196/196 [00:00<00:00, 39197.23 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n",
      "\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/171 05:18 < 2:53:51, 0.02 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m      5\u001b[39m sft_config = SFTConfig(\n\u001b[32m      6\u001b[39m     output_dir=OUTPUT_DIR,\n\u001b[32m      7\u001b[39m     per_device_train_batch_size=\u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     packing=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m trainer = SFTTrainer(\n\u001b[32m     28\u001b[39m     model=model,\n\u001b[32m     29\u001b[39m     args=sft_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     processing_class=tokenizer,\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m trainer.save_model(OUTPUT_DIR)\n\u001b[32m     37\u001b[39m tokenizer.save_pretrained(OUTPUT_DIR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:1190\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\accelerate\\accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\finetune\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "OUTPUT_DIR = \"./qwen25_resume_lora\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=20,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=2048,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"val\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35ed86",
   "metadata": {},
   "source": [
    "# Section 6 — JSON Validation Pipeline\n",
    "Generation outputs often include the prompt, so we trim it, extract the first JSON object, and validate key presence plus types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2297287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_output(resume_text, job_description):\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        resume_text=resume_text,\n",
    "        job_description=job_description,\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if prompt in decoded:\n",
    "        decoded = decoded[len(prompt):].strip()\n",
    "\n",
    "    first_brace = decoded.find(\"{\")\n",
    "    if first_brace > 0:\n",
    "        decoded = decoded[first_brace:]\n",
    "\n",
    "    return decoded\n",
    "\n",
    "def validate_json_structure(obj):\n",
    "    required = [\n",
    "        \"optimized_resume_json\",\n",
    "        \"optimized_resume_text\",\n",
    "        \"ats_score_original\",\n",
    "        \"ats_score_regenerated\",\n",
    "        \"improvement\",\n",
    "    ]\n",
    "    for key in required:\n",
    "        if key not in obj:\n",
    "            return False\n",
    "    if not isinstance(obj[\"optimized_resume_json\"], dict):\n",
    "        return False\n",
    "    if not isinstance(obj[\"optimized_resume_text\"], str):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3732d9",
   "metadata": {},
   "source": [
    "## Section 6.1 — Run Validation on Sample Batch\n",
    "Sample random validation examples, generate outputs, and report whether JSON parsing and structure checks succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "val_data = processed_dataset[\"val\"]\n",
    "indices = list(range(len(val_data)))\n",
    "random.shuffle(indices)\n",
    "indices = indices[:20]\n",
    "\n",
    "for idx in indices:\n",
    "    example = val_data[idx]\n",
    "    generated = generate_output(example[\"resume_text\"], example[\"job_description\"])\n",
    "    try:\n",
    "        parsed = json.loads(generated)\n",
    "        print(\"VALID JSON ✓\" if validate_json_structure(parsed) else \"STRUCTURE FAIL\")\n",
    "    except Exception:\n",
    "        print(\"JSON PARSE FAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8c89d",
   "metadata": {},
   "source": [
    "# Section 7 — Save Model, Tokenizer, and Instructions\n",
    "Exported artifacts in `OUTPUT_DIR` can be converted to GGUF, wrapped in a FastAPI service, or deployed on your preferred inference stack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
